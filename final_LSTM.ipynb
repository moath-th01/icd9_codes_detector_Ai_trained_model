{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Number of labels after filtering: 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 25/25 [00:10<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.0575 - F1 Micro: 0.2064, Macro: 0.0134, average: 0.0874 - \n",
      "Recall Micro: 0.2123, Macro: 0.0339, average: 0.2123 -\n",
      " Precision Micro: 0.2008, Macro: 0.0109, average: 0.0576 - Top-10 Accuracy: 0.8477  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 25/25 [00:09<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.0401 - F1 Micro: 0.2187, Macro: 0.0105, average: 0.0837 - \n",
      "Recall Micro: 0.1886, Macro: 0.0251, average: 0.1886 -\n",
      " Precision Micro: 0.2602, Macro: 0.0069, average: 0.0551 - Top-10 Accuracy: 0.8528  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 25/25 [00:11<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.0396 - F1 Micro: 0.2180, Macro: 0.0153, average: 0.0982 - \n",
      "Recall Micro: 0.2192, Macro: 0.0352, average: 0.2192 -\n",
      " Precision Micro: 0.2168, Macro: 0.0113, average: 0.0684 - Top-10 Accuracy: 0.8579  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 25/25 [00:12<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 0.0392 - F1 Micro: 0.2145, Macro: 0.0175, average: 0.1032 - \n",
      "Recall Micro: 0.2133, Macro: 0.0343, average: 0.2133 -\n",
      " Precision Micro: 0.2156, Macro: 0.0152, average: 0.0783 - Top-10 Accuracy: 0.8477  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 25/25 [00:12<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 0.0382 - F1 Micro: 0.2155, Macro: 0.0151, average: 0.0952 - \n",
      "Recall Micro: 0.2026, Macro: 0.0300, average: 0.2026 -\n",
      " Precision Micro: 0.2302, Macro: 0.0207, average: 0.0849 - Top-10 Accuracy: 0.8629  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 25/25 [00:09<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Loss: 0.0374 - F1 Micro: 0.2104, Macro: 0.0174, average: 0.0983 - \n",
      "Recall Micro: 0.1891, Macro: 0.0292, average: 0.1891 -\n",
      " Precision Micro: 0.2370, Macro: 0.0229, average: 0.0887 - Top-10 Accuracy: 0.8376  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 25/25 [00:09<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Loss: 0.0367 - F1 Micro: 0.1982, Macro: 0.0320, average: 0.1342 - \n",
      "Recall Micro: 0.3111, Macro: 0.0751, average: 0.3111 -\n",
      " Precision Micro: 0.1454, Macro: 0.0230, average: 0.0920 - Top-10 Accuracy: 0.8426  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 25/25 [00:09<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Loss: 0.0356 - F1 Micro: 0.1950, Macro: 0.0205, average: 0.1035 - \n",
      "Recall Micro: 0.1795, Macro: 0.0308, average: 0.1795 -\n",
      " Precision Micro: 0.2136, Macro: 0.0197, average: 0.0831 - Top-10 Accuracy: 0.8426  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 25/25 [00:09<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Loss: 0.0345 - F1 Micro: 0.2062, Macro: 0.0273, average: 0.1173 - \n",
      "Recall Micro: 0.2117, Macro: 0.0415, average: 0.2117 -\n",
      " Precision Micro: 0.2010, Macro: 0.0366, average: 0.1074 - Top-10 Accuracy: 0.8528  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Load dataset and filter rare labels\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"rntc/mimic-icd-visit\", split='train')\n",
    "df = ds.to_pandas()\n",
    "df = df.sample(n=17000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "label_counts = Counter(code for codes in df['icd_code'] for code in codes)\n",
    "min_count = 10\n",
    "common_labels = {label for label, count in label_counts.items() if count >= min_count}\n",
    "df['icd_code'] = df['icd_code'].apply(lambda codes: [code for code in codes if code in common_labels])\n",
    "df = df[df['icd_code'].map(len) > 0].reset_index(drop=True)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['icd_code'])\n",
    "classes = mlb.classes_\n",
    "print(f\"Number of labels after filtering: {len(classes)}\")\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['cleaned_text'].tolist(), y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Tokenizer and Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "word2idx = {word: idx for word, idx in vocab.items()}\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer.tokenize(self.texts[idx])[:self.max_len]\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        pad_len = self.max_len - len(ids)\n",
    "        input_ids = torch.tensor(ids + [0]*pad_len, dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return input_ids, label\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 3. Model with increased capacity\n",
    "class LSTMMultiLabel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, n_layers=3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,\n",
    "                            num_layers=n_layers, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        if self.lstm.bidirectional:\n",
    "            h_n = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_n = h_n[-1]\n",
    "        h_n = self.dropout(h_n)\n",
    "        return self.classifier(h_n)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMMultiLabel(len(word2idx), embed_dim=300, hidden_dim=512, num_labels=len(classes)).to(device)\n",
    "\n",
    "# 4. Focal Loss implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            BCE_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "criterion = FocalLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "best_f1 = 0\n",
    "for epoch in range(9):  # epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = torch.sigmoid(model(inputs)).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_true.append(labels.numpy())\n",
    "\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_true = np.vstack(all_true)\n",
    "\n",
    "    # Tune threshold between 0.3 and 0.7 for best micro F1\n",
    "    best_threshold = 0.5\n",
    "    best_val_f1_micro = 0\n",
    "    for t in np.linspace(0.3, 0.7, 9):\n",
    "        preds_bin = (all_preds > t).astype(int)\n",
    "        f1_micro = f1_score(all_true, preds_bin, average='micro', zero_division=0)\n",
    "        if f1_micro > best_val_f1_micro:\n",
    "            best_val_f1_micro = f1_micro\n",
    "            best_threshold = t\n",
    "\n",
    "    final_preds = (all_preds > best_threshold).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    f1_micro = f1_score(all_true, final_preds, average='micro', zero_division=0)\n",
    "    recall_micro = recall_score(all_true, final_preds, average='micro', zero_division=0)\n",
    "    precision_micro = precision_score(all_true, final_preds, average='micro', zero_division=0)\n",
    "\n",
    "    f1_macro = f1_score(all_true, final_preds, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(all_true, final_preds, average='macro', zero_division=0)\n",
    "    precision_macro = precision_score(all_true, final_preds, average='macro', zero_division=0)\n",
    "\n",
    "    f1_weighted = f1_score(all_true, final_preds, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(all_true, final_preds, average='weighted', zero_division=0)\n",
    "    precision_weighted = precision_score(all_true, final_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    # accuracy = (final_preds == all_true).all(axis=1).mean()\n",
    "\n",
    "    # Top-k accuracy (top_k=10)\n",
    "    top_k = 10\n",
    "    topk_indices = np.argsort(all_preds, axis=1)[:, -top_k:]\n",
    "    correct_topk = 0\n",
    "    for i in range(all_true.shape[0]):\n",
    "        true_indices = set(np.where(all_true[i] == 1)[0])\n",
    "        pred_indices = set(topk_indices[i])\n",
    "        if len(true_indices.intersection(pred_indices)) > 0:\n",
    "            correct_topk += 1\n",
    "    topk_accuracy = correct_topk / all_true.shape[0]\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f} - \"\n",
    "          f\"F1 Micro: {f1_micro:.4f}, Macro: {f1_macro:.4f}, average: {f1_weighted:.4f} - \\n\"\n",
    "          f\"Recall Micro: {recall_micro:.4f}, Macro: {recall_macro:.4f}, average: {recall_weighted:.4f} -\\n \"\n",
    "          f\"Precision Micro: {precision_micro:.4f}, Macro: {precision_macro:.4f}, average: {precision_weighted:.4f} - \"\n",
    "          f\"Top-{top_k} Accuracy: {topk_accuracy:.4f}  \"\n",
    "          )\n",
    "\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    if best_val_f1_micro > best_f1:\n",
    "        best_f1 = best_val_f1_micro\n",
    "        torch.save(model.state_dict(), \"best_lstm_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels_training = 3813  # for example, the number from your checkpoint training\n",
    "model = LSTMMultiLabel(\n",
    "    vocab_size=len(word2idx),\n",
    "    embed_dim=300,\n",
    "    hidden_dim=512,\n",
    "    num_labels=num_labels_training,\n",
    "    n_layers=3,\n",
    "    bidirectional=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMMultiLabel(\n",
       "  (embedding): Embedding(28996, 300)\n",
       "  (lstm): LSTM(300, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=3813, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_lstm_model.pt', map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels above threshold: ['A047', 'A0471', 'A0472', 'A0811', 'A09', 'A310', 'A408', 'A4101', 'A4102', 'A411', 'A4151', 'A4152', 'A4159', 'A419', 'A6000', 'A879', 'B009', 'B0229', 'B029', 'B181', 'B182', 'B1910', 'B258', 'B259', 'B351', 'B356', 'B369', 'B370', 'B3749', 'B377', 'B3789', 'B600', 'B952', 'B955', 'B9562', 'B957', 'B958', 'B9629', 'B966', 'B9681', 'B9789', 'C155', 'C159', 'C160', 'C189', 'C220', 'C221', 'C23', 'C251', 'C252', 'C3401', 'C3412', 'C342', 'C3491', 'C3492', 'C50912', 'C539', 'C55', 'C562', 'C641', 'C642', 'C649', 'C679', 'C710', 'C711', 'C719', 'C779', 'C7800', 'C7802', 'C782', 'C784', 'C785', 'C786', 'C787', 'C7889', 'C7911', 'C7951', 'C7B8', 'C8339', 'C8519', 'C8599', 'C9002', 'C9100', 'C9110', 'C9200', 'C9202', 'C9210', 'C946', 'D120', 'D123', 'D124', 'D125', 'D259', 'D270', 'D271', 'D3502', 'D45', 'D469', 'D473', 'D47Z1', 'D508', 'D539', 'D5700', 'D573', 'D589', 'D594', 'D599', 'D61810', 'D61818', 'D630', 'D631', 'D638', 'D6489', 'D649', 'D66', 'D680', 'D6859', 'D6862', 'D693', 'D6959', 'D696', 'D709', 'D735', 'D8689', 'D89813', 'E039', 'E041', 'E049', 'E0500', 'E0520', 'E0580', 'E063', 'E079', 'E099', 'E10610', 'E10649', 'E1065', 'E1069', 'E109', 'E1100', 'E1110', 'E1121', 'E11319', 'E1139', 'E1142', 'E11621', 'E11622', 'E1165', 'E118', 'E119', 'E1310', 'E139', 'E212', 'E222', 'E232', 'E273', 'E2740', 'E2749', 'E279', 'E291', 'E440', 'E512', 'E538', 'E559', 'E662', 'E663', 'E669', 'E739', 'E7800', 'E781', 'E782', 'E784', 'E7849', 'E785', 'E790', 'E806', 'E83119', 'E8339', 'E8352', 'E8359', 'E860', 'E869', 'E870', 'E871', 'E872', 'E874', 'E8770', 'E8779', 'E878', 'E8809', 'E8881', 'F0150', 'F0151', 'F0280', 'F0281', 'F09', 'F1010', 'F1011', 'F10129', 'F1021', 'F10231', 'F10239', 'F1099', 'F1110', 'F1111', 'F1120', 'F1123', 'F1190', 'F1411', 'F1420', 'F1490', 'F1510', 'F17210', 'F17290', 'F1911', 'F200', 'F22', 'F29', 'F3181', 'F319', 'F329', 'F341', 'F40240', 'F410', 'F411', 'F4310', 'F4321', 'F4323', 'F459', 'F5000', 'F509', 'F602', 'F609', 'F819', 'F909', 'G060', 'G061', 'G08', 'G20', 'G251', 'G252', 'G2581', 'G309', 'G35', 'G40109', 'G40209', 'G40219', 'G40409', 'G40802', 'G4089', 'G40901', 'G43109', 'G43809', 'G44209', 'G4440', 'G4731', 'G4733', 'G500', 'G510', 'G608', 'G609', 'G6289', 'G7000', 'G7281', 'G8220', 'G8250', 'G8321', 'G834', 'G8911', 'G8918', 'G8921', 'G894', 'G901', 'G911', 'G919', 'G92', 'G931', 'G9340', 'G9341', 'G9349', 'G9382', 'G939', 'G9519', 'G9589', 'G960', 'G9741', 'H02401', 'H04129', 'H109', 'H3530', 'H53461', 'H53462', 'H5347', 'H5440', 'H5441', 'H5461', 'H547', 'H548', 'H8109', 'H8110', 'H81399', 'H903', 'H905', 'H9190', 'H9192', 'H9193', 'I080', 'I110', 'I129', 'I130', 'I132', 'I200', 'I2129', 'I213', 'I214', 'I21A1', 'I237', 'I240', 'I2510', 'I25119', 'I253', 'I255', 'I25710', 'I25810', 'I2584', 'I272', 'I2721', 'I2723', 'I2729', 'I314', 'I319', 'I340', 'I351', 'I361', 'I420', 'I421', 'I43', 'I442', 'I447', 'I4589', 'I462', 'I469', 'I471', 'I472', 'I481', 'I482', 'I491', 'I495', 'I498', 'I499', 'I5020', 'I5021', 'I5023', 'I5041', 'I5043', 'I517', 'I5181', 'I519', 'I608', 'I609', 'I610', 'I618', 'I6200', 'I6201', 'I6340', 'I63412', 'I6349', 'I6389', 'I6522', 'I6789', 'I680', 'I69319', 'I69322', 'I69328', 'I69351', 'I69354', 'I69392', 'I69398', 'I700', 'I70201', 'I70209', 'I70235', 'I70261', 'I708', 'I7092', 'I7101', 'I723', 'I724', 'I7300', 'I739', 'I742', 'I745', 'I748', 'I76', 'I776', 'I7774', 'I82411', 'I82412', 'I82432', 'I82441', 'I824Z2', 'I82512', 'I82611', 'I82621', 'I82891', 'I8500', 'I8511', 'I864', 'I871', 'I872', 'I878', 'I890', 'I952', 'I953', 'I9581', 'I9589', 'I959', 'I97190', 'I97191', 'I97618', 'I9788', 'I9789', 'I998', 'J029', 'J0410', 'J069', 'J1000', 'J101', 'J129', 'J150', 'J151', 'J15211', 'J156', 'J181', 'J188', 'J189', 'J208', 'J329', 'J3489', 'J383', 'J386', 'J398', 'J40', 'J439', 'J440', 'J4530', 'J45909', 'J45990', 'J471', 'J479', 'J61', 'J690', 'J704', 'J80', 'J810', 'J811', 'J8489', 'J869', 'J910', 'J918', 'J939', 'J95811', 'J95821', 'J95830', 'J95851', 'J9600', 'J9602', 'J9610', 'J9611', 'J9620', 'J9809', 'J9811', 'J984', 'K029', 'K047', 'K0889', 'K120', 'K122', 'K1230', 'K220', 'K226', 'K228', 'K254', 'K259', 'K264', 'K279', 'K289', 'K2960', 'K2980', 'K30', 'K311', 'K315', 'K31811', 'K3189', 'K319', 'K3580', 'K37', 'K432', 'K439', 'K449', 'K469', 'K5080', 'K521', 'K551', 'K562', 'K5641', 'K565', 'K5650', 'K56609', 'K5669', 'K5710', 'K5731', 'K5791', 'K5792', 'K580', 'K589', 'K5903', 'K5909', 'K598', 'K621', 'K625', 'K626', 'K631', 'K632', 'K633', 'K635', 'K6389', 'K648', 'K649', 'K652', 'K658', 'K660', 'K661', 'K7010', 'K7011', 'K7040', 'K709', 'K7290', 'K740', 'K743', 'K7581', 'K760', 'K761', 'K766', 'K767', 'K7689', 'K8000', 'K8012', 'K8030', 'K8031', 'K8050', 'K8070', 'K8080', 'K811', 'K822', 'K830', 'K8309', 'K831', 'K851', 'K8510', 'K852', 'K8520', 'K8590', 'K861', 'K862', 'K863', 'K868', 'K8681', 'K8689', 'K869', 'K900', 'K913', 'K9171', 'K91840', 'K91850', 'K9186', 'K91870', 'K921', 'K922', 'K9413', 'K9419', 'K9423', 'K9589', 'L02211', 'L0231', 'L02413', 'L02414', 'L02415', 'L02416', 'L02611', 'L03011', 'L03116', 'L03311', 'L03313', 'L089', 'L120', 'L219', 'L298', 'L299', 'L400', 'L4050', 'L539', 'L570', 'L719', 'L732', 'L7622', 'L7634', 'L7682', 'L821', 'L88', 'L89150', 'L89154', 'L97229', 'L97419', 'L97429', 'L97511', 'L97521', 'L97529', 'L97829', 'L97919', 'L982', 'L988', 'L989', 'M009', 'M069', 'M1611', 'M170', 'M1710', 'M1711', 'M1712', 'M19011', 'M19012', 'M19042', 'M1990', 'M21372', 'M25511', 'M25512', 'M25519', 'M25552', 'M25559', 'M25561', 'M25562', 'M311', 'M329', 'M3500', 'M40209', 'M419', 'M4316', 'M4624', 'M4726', 'M47816', 'M47892', 'M479', 'M4856XA', 'M5126', 'M5136', 'M5410', 'M5412', 'M5440', 'M545', 'M5481', 'M549', 'M6281', 'M6282', 'M659', 'M75102', 'M791', 'M7910', 'M79602', 'M79605', 'M79661', 'M79671', 'M79672', 'M7989', 'M8088XA', 'M810', 'M86171', 'M86671', 'M868X7', 'M87852', 'M899', 'N028', 'N049', 'N12', 'N131', 'N132', 'N1339', 'N135', 'N136', 'N139', 'N179', 'N181', 'N184', 'N186', 'N189', 'N210', 'N2581', 'N2589', 'N289', 'N3090', 'N359', 'N3641', 'N3941', 'N39490', 'N400', 'N419', 'N529', 'N62', 'N7011', 'N7093', 'N719', 'N739', 'N809', 'N814', 'N823', 'N83201', 'N838', 'N920', 'N939', 'N950', 'N990', 'N99840', 'N9989', 'O09523', 'O26892', 'O26893', 'O34211', 'O76', 'O9081', 'O9089', 'O99012', 'O99013', 'O99281', 'O99343', 'O99824', 'O9989', 'Q059', 'Q231', 'Q2733', 'Q613', 'Q796', 'R000', 'R002', 'R008', 'R030', 'R0489', 'R0602', 'R0609', 'R062', 'R0682', 'R0689', 'R0902', 'R0989', 'R1011', 'R1013', 'R1030', 'R1032', 'R1084', 'R110', 'R1311', 'R1312', 'R1319', 'R159', 'R160', 'R161', 'R17', 'R195', 'R197', 'R200', 'R221', 'R252', 'R262', 'R2689', 'R270', 'R278', 'R29702', 'R29703', 'R29704', 'R29810', 'R3129', 'R338', 'R350', 'R351', 'R3911', 'R3915', 'R400', 'R402112', 'R402132', 'R402142', 'R402212', 'R402252', 'R402362', 'R4189', 'R419', 'R441', 'R443', 'R451', 'R45851', 'R4701', 'R4789', 'R5081', 'R51', 'R55', 'R569', 'R570', 'R579', 'R591', 'R599', 'R601', 'R609', 'R6250', 'R627', 'R634', 'R6521', 'R6881', 'R710', 'R7309', 'R740', 'R748', 'R7611', 'R791', 'R7989', 'R824', 'R8271', 'R938', 'R9431', 'R944', 'R945', 'S0081XA', 'S01111A', 'S01511A', 'S0181XA', 'S022XXA', 'S0232XA', 'S02401A', 'S0240FA', 'S0266XA', 'S065X0A', 'S065X9A', 'S066X0A', 'S066X9A', 'S12600A', 'S22089A', 'S2220XA', 'S2232XA', 'S270XXA', 'S300XXA', 'S301XXA', 'S32019A', 'S32039A', 'S32049A', 'S3210XA', 'S42211A', 'S42212A', 'S61412A', 'S72001A', 'T17890A', 'T17920A', 'T17990A', 'T3695XA', 'T375X5A', 'T378X5A', 'T380X5S', 'T383X5A', 'T383X6A', 'T39395A', 'T402X5A', 'T404X5A', 'T40605A', 'T41295A', 'T424X5A', 'T426X5A', 'T426X6A', 'T43215A', 'T43225A', 'T434X5A', 'T43595A', 'T447X5A', 'T450X5A', 'T461X5A', 'T465X5A', 'T466X5A', 'T473X6A', 'T500X5A', 'T501X5A', 'T50905A', 'T801XXA', 'T8119XA', 'T8132XA', 'T8143XA', 'T8144XA', 'T82524A', 'T82855A', 'T82856A', 'T82857A', 'T83022A', 'T83511A', 'T8351XA', 'T8453XA', 'T8469XA', 'T8484XA', 'T85898A', 'T8611', 'T8619', 'T8649', 'T865', 'T8744', 'V4352XA', 'V475XXA', 'V892XXA', 'V892XXS', 'W000XXA', 'W010XXA', 'W01190A', 'W108XXA', 'W1809XA', 'W1811XA', 'W1830XA', 'W19XXXA', 'W19XXXS', 'W312XXA', 'W5501XA', 'X58XXXA', 'Y042XXA', 'Y658', 'Y712', 'Y792', 'Y828', 'Y831', 'Y836', 'Y839', 'Y841', 'Y844', 'Y906', 'Y908', 'Y92002', 'Y92009', 'Y92013', 'Y92019', 'Y92031', 'Y92038', 'Y92098', 'Y92099', 'Y92199', 'Y92231', 'Y92234', 'Y92410', 'Y9269', 'Y9289', 'Z006', 'Z09', 'Z1211', 'Z1611', 'Z1621', 'Z1623', 'Z170', 'Z2239', 'Z23', 'Z370', 'Z3A01', 'Z3A33', 'Z3A37', 'Z3A39', 'Z431', 'Z481', 'Z5111', 'Z516', 'Z5321', 'Z5329', 'Z5331', 'Z560', 'Z590', 'Z62810', 'Z66', 'Z6820', 'Z6821', 'Z6822', 'Z6823', 'Z6824', 'Z6825', 'Z6827', 'Z6829', 'Z6832', 'Z6839', 'Z6841', 'Z6842', 'Z781', 'Z7901', 'Z7902', 'Z792', 'Z794', 'Z7951', 'Z7982', 'Z79891', 'Z79899', 'Z801', 'Z8041', 'Z8042', 'Z8049', 'Z8051', 'Z806', 'Z809', 'Z818', 'Z823', 'Z8249', 'Z8379', 'Z8489', 'Z8501', 'Z8505', 'Z8507', 'Z8509', 'Z8521', 'Z853', 'Z8542', 'Z8543', 'Z8546', 'Z85528', 'Z8579', 'Z85810', 'Z85818', 'Z85820', 'Z85828', 'Z85830', 'Z85831', 'Z86018', 'Z8611', 'Z86718', 'Z87311', 'Z87440', 'Z87442', 'Z8781', 'Z87898', 'Z880', 'Z881', 'Z888', 'Z89422', 'Z89431', 'Z89612', 'Z9010', 'Z902', 'Z90410', 'Z90411', 'Z905', 'Z906', 'Z90722', 'Z9079', 'Z9081', 'Z91041', 'Z91128', 'Z91138', 'Z9114', 'Z9115', 'Z9119', 'Z9181', 'Z930', 'Z931', 'Z934', 'Z936', 'Z944', 'Z947', 'Z9484', 'Z954', 'Z955', 'Z95810', 'Z95811', 'Z95818', 'Z95820', 'Z95828', 'Z960', 'Z961', 'Z9641', 'Z96612', 'Z96642', 'Z96649', 'Z96652', 'Z9689', 'Z975', 'Z980', 'Z981', 'Z9884', 'Z98890', 'Z9911', 'Z992', 'Z993', 'Z9981']\n",
      "Top-k labels: ['M170', 'G43109', 'T41295A', 'C220', 'C55']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Inference function\n",
    "def predict_icd_codes(text, model, tokenizer, classes, max_len=256, threshold=0.5, top_k=5, device='cpu'):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(text)[:max_len]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    pad_len = max_len - len(token_ids)\n",
    "    input_ids = torch.tensor(token_ids + [0]*pad_len, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "\n",
    "    above_threshold = [classes[i] for i in np.where(probs >= threshold)[0]]\n",
    "    topk_indices = probs.argsort()[-top_k:][::-1]\n",
    "    top_k_labels = [classes[i] for i in topk_indices]\n",
    "\n",
    "    return {'above_threshold': above_threshold, 'top_k': top_k_labels}\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Patient admitted with acute myocardial infarction and chest pain.\"\n",
    "\n",
    "result = predict_icd_codes(sample_text, model, tokenizer, classes, max_len=256, threshold=0.5, top_k=5, device=device)\n",
    "\n",
    "print(\"Labels above threshold:\", result['above_threshold'])\n",
    "print(\"Top-k labels:\", result['top_k'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
